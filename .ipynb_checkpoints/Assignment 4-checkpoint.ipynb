{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5439e0-7610-452e-adc2-a9d8b7a67550",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: SOUMINI MOHANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b67a661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2af8bd32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the occupancy dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b5ca721-db45-4643-81e8-d0ac9c7d4a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the occupancy dataset into a feature matrix X and a target vector y\n",
    "X, y = load_occupancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cb91b06-3697-40f9-a21d-30d38ebb9106",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of feature matrix X:  (20560, 5)\n",
      "The type of feature matrix X:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the feature matrix X \n",
    "# Shape indicates (n_samples, n_features)\n",
    "print(\"The shape of feature matrix X: \", X.shape) \n",
    "print(\"The type of feature matrix X: \", type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9c5256c-3103-4a8b-b3d1-d1eb19cc79e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of target vector y:  (20560,)\n",
      "The type of taget vector y:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the target vector y \n",
    "# Shape indicates length (n_samples)\n",
    "print(\"The shape of target vector y: \", y.shape) \n",
    "print(\"The type of taget vector y: \", type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fed6ac60-4876-4ba3-9ed5-4ca5d673e75b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>relative humidity</th>\n",
       "      <th>light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.18</td>\n",
       "      <td>27.2720</td>\n",
       "      <td>426.0</td>\n",
       "      <td>721.25</td>\n",
       "      <td>0.004793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2675</td>\n",
       "      <td>429.5</td>\n",
       "      <td>714.00</td>\n",
       "      <td>0.004783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2450</td>\n",
       "      <td>426.0</td>\n",
       "      <td>713.50</td>\n",
       "      <td>0.004779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.15</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>708.25</td>\n",
       "      <td>0.004772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.10</td>\n",
       "      <td>27.2000</td>\n",
       "      <td>426.0</td>\n",
       "      <td>704.50</td>\n",
       "      <td>0.004757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temperature  relative humidity  light     CO2  humidity\n",
       "0        23.18            27.2720  426.0  721.25  0.004793\n",
       "1        23.15            27.2675  429.5  714.00  0.004783\n",
       "2        23.15            27.2450  426.0  713.50  0.004779\n",
       "3        23.15            27.2000  426.0  708.25  0.004772\n",
       "4        23.10            27.2000  426.0  704.50  0.004757"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the various columns present in the feature matrix X\n",
    "# (i.e., 5 features = 5 columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a11d255-e729-4819-9f36-b01da5e187c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "Name: occupancy, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the 1D target array having only 1 column (i.e., occupancy)\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?\n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec75064-1902-4a86-84d5-315d1f634c7a",
   "metadata": {},
   "source": [
    "Answer 01: The dataset used for this assignment is downloaded from the yellowbrick library. \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/occupancy.html\n",
    "The yellowbrick function load_occupancy() was used to to load the occupancy dataset into the feature matrix X and target vector y. And the size and type of X and y was viewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e822462-ec6e-4e8b-8db7-a64c40caab2c",
   "metadata": {},
   "source": [
    "Answer 02: \n",
    "I chose this dataset for the following reasons:\n",
    "a) The dataset is designed for binary classification, which makes it well-suited for tasks where you want to predict one of two discrete outcomes, such as \"occupied\" or \"not occupied.\"\n",
    "b) With 20,560 instances, the dataset provides a reasonably large sample size.\n",
    "c) While the dataset provides real-valued attributes, there's still room for feature engineering if needed. One can create additional features or apply transformations to improve model performance.\n",
    "d) And I believe the fact that the dataset contains not very many features (just 5) to choose from, makes it quite challenging during feature selection. I wish to see how the results turn out to be in such a scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b971db-472a-4a84-9c63-cf27c2a0f7eb",
   "metadata": {},
   "source": [
    "Answer 03: \n",
    "Discovering a suitable dataset felt like a challenging dilemma. I would come across intriguing ones, only to realize midway through the modeling process that they yielded unsatisfactory results. Conversely, when I encountered datasets that performed well, they lacked the same level of interest. Eventually, I settled on the current dataset because I found it to be both complex and demanding from various angles. \n",
    "Some of the factors that I found challenging with this dataset are: \n",
    "a) One common challenge in binary classification datasets is class imbalance. If one class significantly outnumbers the other (e.g., many more instances of \"not occupied\" than \"occupied\"), it can affect model performance. And this dataset has a class imbalance. \n",
    "b) While the dataset has 5 features, not all of them may be equally informative. Feature selection or feature importance analysis may be necessary to identify the most relevant attributes for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afc244d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temperature          0\n",
       "relative humidity    0\n",
       "light                0\n",
       "CO2                  0\n",
       "humidity             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "# Check if the X dataframe contains any missing or NaN values\n",
    "# sum() indicates the total count of NaN values present in each column \n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcdfe3d4-0c4d-489c-be05-4c2862643918",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the target vector y has any missing values \n",
    "# present in its 1 column (i.e., occupancy)\n",
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32b02f71-d46c-498f-b09c-f711601e9096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c03d548a-e0b6-4568-b147-00edcb2c2120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since the dataset contains imbalanced classes, techniques like oversampling/undersampling, \n",
    "# can be used to handle it. And this is done by resampling the dataset and training the model on the \n",
    "# resampled training data. There are two ways to resample the training data i.e., either oversample the \n",
    "# minority class or undersample the majority class. \n",
    "# In this case, I have opted to oversample the minority class \n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Combine the feature and target data for the training set\n",
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "majority_class = Xy_train[Xy_train['occupancy'] == 0]\n",
    "minority_class = Xy_train[Xy_train['occupancy'] == 1]\n",
    "\n",
    "# Oversample the minority class (e.g., duplicate minority samples)\n",
    "minority_class_oversampled = resample(minority_class,\n",
    "                                      replace=True,  # Sample with replacement\n",
    "                                      n_samples=len(majority_class),  # Match the majority class size\n",
    "                                      random_state=42)  # Set a random seed for reproducibility\n",
    "\n",
    "# Combine the oversampled minority class with the majority class\n",
    "Xy_train_resampled = pd.concat([majority_class, minority_class_oversampled])\n",
    "\n",
    "# Separate the features and target from the resampled data\n",
    "X_train_resampled = Xy_train_resampled.drop('occupancy', axis=1)\n",
    "y_train_resampled = Xy_train_resampled['occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e84e20-a29b-49bf-8fc7-579f5688125c",
   "metadata": {},
   "source": [
    "Answer 01: \n",
    "As seen in the beginning of Step.2, there are no missing or NaN values in either the feature matrix X or the target vector y. Hence it is not necessary to use a method to fill in missing values. If missing values had existed, assuming that we are filling it with Zeros instead of just dropping the row or column containing missing or NaN values, we would have used the below mentioned commands:\n",
    "a) X.fillna(0) # For feature matrix X\n",
    "b) y.fillna(0) # For target vector y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e139ca-4811-4203-b471-a758ff69b63b",
   "metadata": {},
   "source": [
    "Answer 02: \n",
    "a) The data type for the occupancy dataset features is described as \"real\" and \"positive.\" This typically implies that the features are numerical and non-negative.\n",
    "b) Preprocessing methods commonly applied to such data include feature scaling (I have opted to use StandardScaler in the pipeline), handling missing values (there are none in this dataset), and handling imbalanced classes (using techniques like oversampling/undersampling, as needed). This is done by resampling the dataset and training the model on the resampled training data. There are two ways to resample the training data i.e., either oversample the \n",
    "minority class or undersample the majority class. In this case, I have opted to oversample the minority class as seen above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bb06372-6e3c-4243-a427-d22775b4adc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Accuracy scores: [0.99247696 0.99263535 0.99267495 0.99469428 0.99477346 0.99469427\n",
      " 0.99572373 0.99580292 0.99576332]\n",
      "F1 scores: [0.9925118  0.99267036 0.99271064 0.99472133 0.99479896 0.99472025\n",
      " 0.99573899 0.9958179  0.99577823]\n",
      "\n",
      "Best Parameters for Random Forest based on F1: {'classifier__max_depth': 15, 'classifier__n_estimators': 200}\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy scores: [0.99120997 0.99124957 0.99128917 0.99128917 0.99136834 0.99136834]\n",
      "F1 scores: [0.99126703 0.99130514 0.99134486 0.99134486 0.99142264 0.99142271]\n",
      "\n",
      "Best Parameters for Logistic Regression based on F1: {'classifier__C': 10, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
      "\n",
      "SVM Results:\n",
      "Accuracy scores: [0.99140796 0.99132874 0.99124955 0.99223943 0.99113077 0.99279375]\n",
      "F1 scores: [0.99146175 0.99138566 0.99130203 0.99228818 0.99118619 0.99283522]\n",
      "\n",
      "Best Parameters for SVM based on F1: {'classifier__C': 10, 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Logistic Regression Pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# SVM Pipeline\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [5, 10, 15]\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__penalty': ['l1'],  # Use 'l1' penalty\n",
    "    'classifier__solver': ['liblinear', 'saga']  # Choose an appropriate solver\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Define the scoring metrics you want to use\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_score': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "# Create GridSearchCV instances for each algorithm with multiple scoring metrics\n",
    "grid_search_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=5, scoring=scoring, refit='f1_score')\n",
    "grid_search_lr = GridSearchCV(lr_pipeline, param_grid_lr, cv=5, scoring=scoring, refit='f1_score')\n",
    "grid_search_svm = GridSearchCV(svm_pipeline, param_grid_svm, cv=5, scoring=scoring, refit='f1_score')\n",
    "\n",
    "# Fit the models\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_lr.fit(X_train_resampled, y_train_resampled)\n",
    "grid_search_svm.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters based on F1\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "best_params_svm = grid_search_svm.best_params_\n",
    "\n",
    "# Access the results for both scoring metrics\n",
    "results_rf = grid_search_rf.cv_results_\n",
    "results_lr = grid_search_lr.cv_results_\n",
    "results_svm = grid_search_svm.cv_results_\n",
    "\n",
    "# Print the results for accuracy and F1\n",
    "print(\"Random Forest Results:\")\n",
    "print(\"Accuracy scores:\", results_rf['mean_test_accuracy'])\n",
    "print(\"F1 scores:\", results_rf['mean_test_f1_score'])\n",
    "print(\"\\nBest Parameters for Random Forest based on F1:\", best_params_rf)\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(\"Accuracy scores:\", results_lr['mean_test_accuracy'])\n",
    "print(\"F1 scores:\", results_lr['mean_test_f1_score'])\n",
    "print(\"\\nBest Parameters for Logistic Regression based on F1:\", best_params_lr)\n",
    "\n",
    "print(\"\\nSVM Results:\")\n",
    "print(\"Accuracy scores:\", results_svm['mean_test_accuracy'])\n",
    "print(\"F1 scores:\", results_svm['mean_test_f1_score'])\n",
    "print(\"\\nBest Parameters for SVM based on F1:\", best_params_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d856aa-ade3-451f-9151-40eb94107d07",
   "metadata": {},
   "source": [
    "After the grid search, the best model parameters for each model are: \n",
    "a)Best Parameters for Random Forest based on F1: classifier__max_depth = 15, classifier__n_estimators = 300\n",
    "b)Best Parameters for Logistic Regression based on F1: classifier__C = 10, classifier__penalty = l1, classifier__solver = saga\n",
    "c) Best Parameters for SVM based on F1: classifier__C = 10, classifier__kernel = rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9f1824a-53c6-4801-be15-abe8dc100600",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Mean Test Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9924769645733864, 0.9926353490517075, 0.992...</td>\n",
       "      <td>0.983467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.9912099749616461, 0.9912495710812264, 0.991...</td>\n",
       "      <td>0.974251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>[0.9914079555595476, 0.9913287398072519, 0.991...</td>\n",
       "      <td>0.977380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Algorithm                                 Mean Test Accuracy  \\\n",
       "0        Random Forest  [0.9924769645733864, 0.9926353490517075, 0.992...   \n",
       "1  Logistic Regression  [0.9912099749616461, 0.9912495710812264, 0.991...   \n",
       "2                  SVM  [0.9914079555595476, 0.9913287398072519, 0.991...   \n",
       "\n",
       "   Test Accuracy  \n",
       "0       0.983467  \n",
       "1       0.974251  \n",
       "2       0.977380  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you already have 'results_rf', 'results_lr', and 'results_svm' from your code\n",
    "# Extract the mean test accuracy and test accuracy scores\n",
    "mean_test_accuracy_rf = results_rf['mean_test_accuracy']\n",
    "mean_test_accuracy_lr = results_lr['mean_test_accuracy']\n",
    "mean_test_accuracy_svm = results_svm['mean_test_accuracy']\n",
    "\n",
    "test_accuracy_rf = grid_search_rf.score(X_test, y_test)\n",
    "test_accuracy_lr = grid_search_lr.score(X_test, y_test)\n",
    "test_accuracy_svm = grid_search_svm.score(X_test, y_test)\n",
    "\n",
    "# Create a dataframe for the heatmap\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Algorithm': ['Random Forest', 'Logistic Regression', 'SVM'],\n",
    "    'Mean Test Accuracy': [mean_test_accuracy_rf, mean_test_accuracy_lr, mean_test_accuracy_svm],\n",
    "    'Test Accuracy': [test_accuracy_rf, test_accuracy_lr, test_accuracy_svm]\n",
    "}\n",
    "\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f36d8de5-eacc-4282-a71a-ce717a294d61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Mean F1 Test Accuracy</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>[0.9925118047660547, 0.9926703592215838, 0.992...</td>\n",
       "      <td>0.983467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>[0.9912670272790471, 0.9913051372994174, 0.991...</td>\n",
       "      <td>0.974251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>[0.9914617508956717, 0.991385659071498, 0.9913...</td>\n",
       "      <td>0.977380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Algorithm                              Mean F1 Test Accuracy  \\\n",
       "0        Random Forest  [0.9925118047660547, 0.9926703592215838, 0.992...   \n",
       "1  Logistic Regression  [0.9912670272790471, 0.9913051372994174, 0.991...   \n",
       "2                  SVM  [0.9914617508956717, 0.991385659071498, 0.9913...   \n",
       "\n",
       "    Test F1  \n",
       "0  0.983467  \n",
       "1  0.974251  \n",
       "2  0.977380  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you already have 'results_rf', 'results_lr', and 'results_svm' from your code\n",
    "# Extract the mean F1 test accuracy and F1 test accuracy scores\n",
    "mean_test_f1_score_rf = results_rf['mean_test_f1_score']\n",
    "mean_test_f1_score_lr = results_lr['mean_test_f1_score']\n",
    "mean_test_f1_score_svm = results_svm['mean_test_f1_score']\n",
    "\n",
    "test_f1_score_rf = grid_search_rf.score(X_test, y_test)\n",
    "test_f1_score_lr = grid_search_lr.score(X_test, y_test)\n",
    "test_f1_score_svm = grid_search_svm.score(X_test, y_test)\n",
    "\n",
    "# Create a dataframe for the heatmap\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Algorithm': ['Random Forest', 'Logistic Regression', 'SVM'],\n",
    "    'Mean F1 Test Accuracy': [mean_test_f1_score_rf, mean_test_f1_score_lr, mean_test_f1_score_svm],\n",
    "    'Test F1': [test_f1_score_rf, test_f1_score_lr, test_f1_score_svm]\n",
    "}\n",
    "\n",
    "df_ = pd.DataFrame(data)\n",
    "df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b418c-8306-4377-8f15-af01e74b9c65",
   "metadata": {},
   "source": [
    "Answer 01: \n",
    "The problem we are addressing with this dataset is binary classification (occupancy), so classification models are appropriate for this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f7d51-7d00-455b-85b3-459d2aa99c97",
   "metadata": {},
   "source": [
    "Answer 02: \n",
    "These three models well-established, versatile algorithms suitable for binary classification tasks.\n",
    "Random Forest was chosen for its ability to capture complex relationships. \n",
    "Logistic Regression was chosen for its interpretability and efficiency. \n",
    "SVM was chosen for its effectiveness in high-dimensional spaces and flexibility in modeling decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c0bfd-5329-474b-9683-a16d0ceae048",
   "metadata": {},
   "source": [
    "Answer 03: \n",
    "It appears that all three models are performing exceptionally well with high accuracy scores. \n",
    "The occupancy dataset is a binary classification problem where the goal is to predict whether a room is occupied or not occupied based on environmental factors such as temperature, humidity, light, and CO2 levels. The dataset likely contains distinct patterns and relationships between these features and room occupancy, which the models are capturing effectively. \n",
    "The preprocessing steps (StandardScaler, handling imbalanced classes) can help improve model performance and contribute to the high test accuracies.\n",
    "Fine-tuning hyperparameters can significantly improve model performance, leading to high test accuracies. \n",
    "If the dataset is well-preprocessed and free of anomalies, it can contribute to better model performance.\n",
    "Balancing the classes can lead to better model generalization.\n",
    "This suggests that the models have learned the underlying patterns in the data effectively and are capable of accurately predicting room occupancy based on environmental factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6453b8-0065-4568-92d8-567f32227a42",
   "metadata": {},
   "source": [
    "Answer 01:\n",
    "Since this dataset is imbalanced, metrics like F1 score, precision, recall, and the confusion matrix can be used to get a more comprehensive view of model performance. And I have used F1 score in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a68b75-72df-4431-a24f-531c2048a493",
   "metadata": {},
   "source": [
    "Answer 02:\n",
    "The test accuracy and F1 score for the models are almost identical. Since the model performs well on both training and testing sets, it is more likely to generalize well. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "62ed7f15-a290-4d58-866b-4a2021f960f1",
   "metadata": {},
   "source": [
    "Answer 03: Whether the best model performs \"well enough\" for real-world use depends on the specific application and requirements. A high F1 score and accuracy suggest good performance, but it's essential to consider the consequences of false positives and false negatives in the context of this application. There is room for improvement and the following can be considered for it: \n",
    "Performing more extensive model evaluation, including examining precision, recall, and the confusion matrix.\n",
    "Feature importance invesstigation to understand which features contribute most to the predictions.\n",
    "Consideration of fine-tuning hyperparameters further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054f0b4-7755-4e13-bb02-e9d351c41b2f",
   "metadata": {},
   "source": [
    "This assignment was open-ended, allowing us to select both the dataset and various linear and non-linear models. I found this task to be simultaneously challenging, intriguing, and occasionally stressful. I discovered that meticulous planning couldn't always prevent encountering unsatisfactory results during the machine learning modeling process, leading to multiple dataset changes in search of one that was both interesting and demanding. Some guidance in narrowing down our choices for dataset selection, as well as linear and non-linear model selection, would have been beneficial. Overall, I believe the experience was valuable, despite the somewhat arduous path to achieving success. Despite its challenges, it piqued my interest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
